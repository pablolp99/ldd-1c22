{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b82a559-0f50-4afc-9150-bba5273e3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89193184-3898-48dd-aeb9-cb3c5d2161d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1265839c-e8b5-4b23-aa56-98ffa366f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(s):\n",
    "    _s = \"\".join([w for w in s.lower() if w not in string.punctuation])\n",
    "    return [i.lower() for i in word_tokenize(_s)]\n",
    "\n",
    "def vectorize(model, ts):\n",
    "    return sum([model.wv.get_vector(i) for i in ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11c199d0-b864-4765-a94d-a76e89287b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TO_DROP = [\n",
    "    'number',\n",
    "    'street',\n",
    "    'side',\n",
    "    'city',\n",
    "    'county',\n",
    "    'state',\n",
    "    'zipcode',\n",
    "    'country',\n",
    "    'timezone',\n",
    "    'airport_code',\n",
    "    'weather_timestamp',\n",
    "]\n",
    "\n",
    "TO_CAST = [\n",
    "    \"amenity\",\n",
    "    \"bump\",\n",
    "    \"crossing\",\n",
    "    \"give_way\",\n",
    "    \"junction\",\n",
    "    \"no_exit\",\n",
    "    \"railway\",\n",
    "    \"roundabout\",\n",
    "    \"station\",\n",
    "    \"stop\",\n",
    "    \"traffic_calming\",\n",
    "    \"traffic_signal\",\n",
    "    \"turning_loop\",\n",
    "    \"sunrise_sunset\",\n",
    "    \"civil_twilight\",\n",
    "    \"nautical_twilight\",\n",
    "    \"astronomical_twilight\",\n",
    "]\n",
    "\n",
    "SS_TW = {\n",
    "    \"Day\": 1,\n",
    "    \"Night\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "44dd2fc7-8a3b-4162-9db2-8f8b03a3d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\").drop(columns=['Unnamed: 0.1','Unnamed: 0'])\n",
    "df_cpy = df.copy()\n",
    "\n",
    "def preprocess(df, mode='train'):\n",
    "    df.columns = list(map(lambda x: x.lower(), df.columns))\n",
    "    df = df.drop(columns=TO_DROP)\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "\n",
    "    df['start_month'] = df['start_time'].apply(lambda x: x.month).astype(np.uint8)\n",
    "    df['start_day'] = df['start_time'].apply(lambda x: x.day).astype(np.uint8)\n",
    "    df['start_hour'] = df['start_time'].apply(lambda x: x.hour).astype(np.uint8)\n",
    "    df['end_month'] = df['end_time'].apply(lambda x: x.month).astype(np.uint8)\n",
    "    df['end_day'] = df['end_time'].apply(lambda x: x.day).astype(np.uint8)\n",
    "    df['end_hour'] = df['end_time'].apply(lambda x: x.hour).astype(np.uint8)\n",
    "\n",
    "    for c in ['sunrise_sunset','civil_twilight','nautical_twilight','astronomical_twilight']:\n",
    "        df[c] = df[c].apply(lambda x: SS_TW.get(x, -1))\n",
    "\n",
    "    for i in TO_CAST:\n",
    "        df[i] = df[i].astype(np.uint8)\n",
    "\n",
    "    if mode == 'train':\n",
    "        pca_weat = PCA(n_components=10)\n",
    "        pca_weat = pca_weat.fit_transform(pd.get_dummies(df.weather_condition, prefix='weather'))\n",
    "        pca_wind = PCA(n_components=10)\n",
    "        pca_wind = pca_wind.fit_transform(pd.get_dummies(df.wind_direction, prefix='weather'))\n",
    "    if mode == 'test':\n",
    "        pass\n",
    "    \n",
    "    for c in pca_weat:\n",
    "        for j, v in enumerate(c):\n",
    "            df[f'weather_{j}'] = c[j]\n",
    "            \n",
    "    for c in pca_wind:\n",
    "        for j, v in enumerate(c):\n",
    "            df[f'wind_{j}'] = c[j]\n",
    "    \n",
    "    df = df.drop(columns=['start_time', 'end_time', 'weather_condition', 'wind_direction'])\n",
    "\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"distance(mi)\": \"distance_mi\",\n",
    "            \"temperature(f)\": \"temperature_f\",\n",
    "            \"wind_chill(f)\": \"wind_chill_f\",\n",
    "            \"humidity(%)\": \"humidity_perc\",\n",
    "            \"pressure(in)\": \"pressure_in\",\n",
    "            \"visibility(mi)\": \"visibility_mi\",\n",
    "            \"wind_speed(mph)\": \"wind_speed_mph\",\n",
    "            \"precipitation(in)\": \"precipitation_in\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns='description')\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "07668be4-2f5a-48c5-a39b-72f9cd1fd347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\").drop(columns=['Unnamed: 0.1','Unnamed: 0'])\n",
    "\n",
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2b182b72-6abd-415f-b52e-a27e79f6b45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>severity</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>distance_mi</th>\n",
       "      <th>temperature_f</th>\n",
       "      <th>wind_chill_f</th>\n",
       "      <th>humidity_perc</th>\n",
       "      <th>pressure_in</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_0</th>\n",
       "      <th>wind_1</th>\n",
       "      <th>wind_2</th>\n",
       "      <th>wind_3</th>\n",
       "      <th>wind_4</th>\n",
       "      <th>wind_5</th>\n",
       "      <th>wind_6</th>\n",
       "      <th>wind_7</th>\n",
       "      <th>wind_8</th>\n",
       "      <th>wind_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17501</th>\n",
       "      <td>0</td>\n",
       "      <td>38.527977</td>\n",
       "      <td>-120.442157</td>\n",
       "      <td>38.527977</td>\n",
       "      <td>-120.442157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>27.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177995</td>\n",
       "      <td>-0.062717</td>\n",
       "      <td>-0.072588</td>\n",
       "      <td>-0.013068</td>\n",
       "      <td>-0.069591</td>\n",
       "      <td>-0.070359</td>\n",
       "      <td>-0.023696</td>\n",
       "      <td>0.069226</td>\n",
       "      <td>0.210654</td>\n",
       "      <td>-0.131019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       severity  start_lat   start_lng    end_lat     end_lng  distance_mi  \\\n",
       "17501         0  38.527977 -120.442157  38.527977 -120.442157          0.0   \n",
       "\n",
       "       temperature_f  wind_chill_f  humidity_perc  pressure_in  ...    wind_0  \\\n",
       "17501           45.0          45.0          100.0        27.13  ... -0.177995   \n",
       "\n",
       "         wind_1    wind_2    wind_3    wind_4    wind_5    wind_6    wind_7  \\\n",
       "17501 -0.062717 -0.072588 -0.013068 -0.069591 -0.070359 -0.023696  0.069226   \n",
       "\n",
       "         wind_8    wind_9  \n",
       "17501  0.210654 -0.131019  \n",
       "\n",
       "[1 rows x 56 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc8744-0fed-476a-b8af-f70baea95107",
   "metadata": {},
   "source": [
    "df_train, df_test = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4387cd46-18da-4ab7-a6cd-c749fa52552b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    df['description_tokenized'] = df.description.apply(tokenize_sentence)\n",
    "    \n",
    "    X = df.drop(columns='severity')\n",
    "    y = df.severity\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "    \n",
    "    df_train = X_train.join(y_train)\n",
    "    df_test = X_test.join(y_test)\n",
    "    \n",
    "    tokenized_sent = df_train.description_tokenized\n",
    "\n",
    "    tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
    "\n",
    "    model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n",
    "\n",
    "    def try_vectorize(model, w):\n",
    "        try:\n",
    "            return vectorize(model, w)\n",
    "        except:\n",
    "            return model.infer_vector(doc_words=x, alpha=0.025)\n",
    "\n",
    "    # model.infer_vector(doc_words=tokens_list, steps=20, alpha=0.025)\n",
    "    df_train['description_vectorized'] = df_train.description_tokenized.apply(lambda x: vectorize(model, x))\n",
    "    df_test['description_vectorized'] = df_test.description_tokenized.apply(lambda x: try_vectorize(model, x))\n",
    "    \n",
    "    _tt_df = df_train.description_vectorized.apply(pd.Series)\n",
    "    _ts_df = df_test.description_vectorized.apply(pd.Series)\n",
    "\n",
    "    _tt_df = _tt_df.rename(\n",
    "        columns={\n",
    "            i: f\"desc_{i}\" for i in range(20)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    _ts_df = _ts_df.rename(\n",
    "        columns={\n",
    "            i: f\"desc_{i}\" for i in range(20)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_train = df_train.drop(columns=['description_tokenized', 'description_vectorized', 'description'])\n",
    "    df_test = df_test.drop(columns=['description_tokenized', 'description_vectorized', 'description'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
